# Kafka

kafka是一个分布式的基于发布/订阅模式（也叫观察者模式）的消息队列
## 概念说明
1）Producer ：消息生产者，就是向 kafka broker 发消息的客户端；
2）Consumer ：消息消费者，向 kafka broker 取消息的客户端；
3）Consumer Group （CG）：消费者组，由多个 consumer 组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。 
4）Broker ：一台 kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个 broker
可以容纳多个 topic。
5）Topic ：可以理解为一个队列，生产者和消费者面向的都是一个 topic；
6）Partition：为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，
一个 topic 可以分为多个 partition，每个 partition 是一个有序的队列；小于等于broker数量；
7）Replica：副本，为保证集群中的某个节点发生故障时，该节点上的 partition 数据不丢失，且 kafka 仍然能够继续工作，kafka 提供了副本机制，一个 topic 的每个分区都有若干个副本，一个 leader 和若干个 follower。
8）leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对
象都是 leader。
9）follower：每个分区多个副本中的“从”，实时从 leader 中同步数据，保持和 leader 数据
的同步。leader 发生故障时，某个 follower 会成为新的 follower

log.dirs=/opt/module/kafka/logs数据和日志均在这个目录下

## 数据检索机制
- topic在物理层面以partition为分组，一个topic可以分为若干个partition
- partition还可以分为Segment（一个Segment一个文件），一个partition物理上由多个Segment组成
	- Segment由两个参数：
		- log.segment.bytes：单个segment可容纳的最大数据量，默认是1GB
		- log.segment.ms：kafka在commit一个未写满的segment前，所等待的时间（默认是7天）（如果7天还没有写满一个文件，还是会创建一个新的文件）
- LogSegment文件由两个部分组成，分别为".index"文件和".log"文件，分别表示为Segment索引文件和数据文件
	- partition全局的第一个segment从0开始，后续每个segmet文件名称为上一个segment文件最后一条消息的offset值；根据offset就可以确定数据在哪个segment
	- 数值大小为64位，20位数值字符长度，没有数字用0填充
		- 第一个segment0000000000000000.index；0000000000000000.log
		- 第二个segment0000000000170410.index；0000000000170410.log
		- 第三个segment0000000000239430.index；0000000000239430.log
- 消息都具有固定的物理结构，包括offset（8byte）、消息体大小（4byte）、crc32（4byte）、magic（1byte）、attributes（1byte）、key length（4byte）、key（K byte）、payload（N byte）等字段，可以确定一条消息的大小
	- index文件创建出来就申请了10M的空间，是为了获取连续的存储空间
- 
## 数据安全性
**可以通过设置自动提交时间接近业务处理的时间来避免重复消费；**
**通过设置acks保证生产者**

****
### producer delevery guarantee生产者保证
0. At least one 消息绝对不丢失，但可能重复传输；先发消息，再写日志
1. At most one 消息可能丢失，但绝不会重复传输；先写日志，再传消息
2. Exactly once：每条消息肯定会传输且传输一次；

- producers可以选择ack选项来设置：request.required.acks
	- **acks=0** Producer在ISR中的Leader已成功收到的数据并得到确认后发送下一条Message；可能发送重复消息
	- **acks=1** Producer无需等待来自Broker的确认而继续发送下一批消息；可能丢失消息
	- **acks=all** Producer需要等待ISR中的所有Follower都确认接收到数据后才算一次发送完成，可靠性最高

### ISR机制
- 关键词：
	- AR：Assigned Replicas用来标识副本的全集
		- - AR = OSR+ISR
	- OSR：out-sync Replicas 离开同步队列的follower
	- ISR：in-sync Reolicas 加入同步队列的follower
		- ISR = Leader+没有落后太多的follower
- 主节点挂掉的时候，启用备份节点
	- producer---push-->leader     leader<---pull---follwer
	- Follower每个一段时间去Leader拉去数据，来保证数据的同步
- ISR
	- 主节点挂掉后，在ISR中选举主节点
	- 判断标准
		- 超过10秒没有同步数据
			- replicas.lag.time.max.ms=10000
		- 主副节点相差超过4000条数据
			- replicas.lag.max.messages=4000
	- 脏节点选举
		- kafka采用一种降级措施来处理
		- 选举第一个恢复的node作为leader提供服务，以它的数据为基准，这个措施称为脏leader选举；**如果leader挂掉，最近的节点还有数据没有同步到**
### broker的数据存储机制
- 无论消息是否被消费，kafka都会保留所有的消息，有两种策略可以删除旧数据
	- 基于时间：log.retention.hours=168
	- 基于大小：log.retention.bytes=173741824

### consumer delivery guarantee消费者保证
- 如果消费者将consumer设置为autocommit，consumer一旦读取到数据立即自动commit，如果只讨论这一读取数消息的过程，那么kafka确保了Exactly once。
- 读完消息先commit再处理消息
	- 如果consumer在commit后还没有来得及处理消息就crash了，下次重新工作后就无法服务刚刚已提交但未处理的消息
	- 这就对应于At most once
- 读完消息先处理再commit
	- 如果在处理完消息之后commit之前consumer crash了，下次重新开始工作的时候还会处理刚刚未commit的消息，实际上这个消息已经处理过了
	- 对应At least once
- 如果一定要做到Exactly once，就要协调offset和时机操作的输出
	- 经典做法是引入两阶段提交
- kafka默认保证At least once，并且允许通过设置producer异步提交来实现At most once

#### 两阶段提交来保证数据

## 数据的消费

## kafka优化
### partition数目
#topic在当前broker上的分片个数；
设置每个topic对应的分片数目：
num.partitions=2分片数；replication.factor副本数

- 一般来说，每个partition能处理的吞吐量为几M/s，增加更多的partition以为这
	- 更高的并行度和吞吐
	- 可以扩展更多的consumer（同一个组中的）
	- 若是集群中有较多的broker，则可能更大程度上的利用闲置的broker
	- 但是会造成Zookeeper的更多讯据
	- 额会在kafka中打开更多的文件
- 调整准则
	- 一般来说，如果集群较小，则配置```2*broker```的partition；这里主要考虑之后的扩展，若是集群扩展了一倍，则不用担心有partition不足的现象
	- 如果集群较大（大于12个），则配置```1*broker```数的partition数，因为这里不需要再考虑集群扩展的情况，与broker数相同的partition已经可以应付常规场景；有必要再手动调整
	- 考虑高吞吐量需要并行的consumer数量，调整partition数目，如果是应用场景需要20个（同一个组中的），则设置20个partition
	- 考虑producer所需要的吞吐，调整partition数目（如果producer吞吐非常高，就增加partition的数目）
### Replication factor
- 这个参数决定records复制的数目，建议至少设置2，一般是3，最高设置4
- 更高的replication factor意味着：
	- 更稳定，允许n-1的broker宕机
	- 更多的副本（如果acks=all，则会造成较高的延时）
	- 系统磁盘的使用率会变高
- 调整准则：
	- 以3为起始
	- 如果replication性能成为瓶颈，则建议使用性能更好的broker，而不是降低RF数目
	- 永远不设置RF为1

### 批量写入：
- 为了大幅提高producer写入吞吐量，需要批量写入文件
	- log.flush.interval.messages每当producer写入10000条数据的时候，刷新到磁盘
	- log.flush.interval.ms = 1000每个一秒写到磁盘

### Flume和kafka集成






